{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass back bad predicted patches with higher probably? (i.e. to retrain more on the harder images).\n",
    "\n",
    "Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_model():\n",
    "    model = get_model_bn_do()\n",
    "    model.fit_generator(batches, batches.N, nb_epoch=1, verbose=0,\n",
    "                        validation_data=test_batches, nb_val_samples=test_batches.N)\n",
    "    model.optimizer.lr=0.1\n",
    "    model.fit_generator(batches, batches.N, nb_epoch=4, verbose=0,\n",
    "                        validation_data=test_batches, nb_val_samples=test_batches.N)\n",
    "    model.optimizer.lr=0.01\n",
    "    model.fit_generator(batches, batches.N, nb_epoch=12, verbose=0,\n",
    "                        validation_data=test_batches, nb_val_samples=test_batches.N)\n",
    "    model.optimizer.lr=0.001\n",
    "    model.fit_generator(batches, batches.N, nb_epoch=18, verbose=0,\n",
    "                        validation_data=test_batches, nb_val_samples=test_batches.N)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [fit_model() for i in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"data/\"\n",
    "model_path = path + 'models/'\n",
    "\n",
    "for i,m in enumerate(models):\n",
    "    m.save_weights(model_path+'model-'+str(i)+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix patchy predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTEBOOK NOT COMPLETE - Copyied prediction bits from keras baseline and https://github.com/asmith26/courses/blob/master/deeplearning1/nbs/dogscats-ensemble.ipynb\n",
    "\n",
    "Maybe also add postprocessing \"if patch is (continuously) big (i.e. crop), don't add to submission\"?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ens_model = vgg_ft(2)\n",
    "for layer in ens_model.layers: layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ens_pred(arr, fname):\n",
    "    ens_pred = []\n",
    "    for i in range(5):\n",
    "        i = str(i)\n",
    "        ens_model.load_weights('{}{}{}.h5'.format(model_path, fname, i))\n",
    "        preds = ens_model.predict(arr, batch_size=batch_size)\n",
    "        ens_pred.append(preds)\n",
    "    return ens_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "al_pred2 = get_ens_pred(val, 'aug')\n",
    "val_avg_preds2 = np.stack(val_pred2).mean(axis=0)\n",
    "categorical_accuracy(val_labels, val_avg_preds2).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prd, score, trs = calc_jacc(model)\n",
    "print('val jk', score)\n",
    "print('trs', trs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mask_for_polygons(polygons, im_size):\n",
    "    # __author__ = Konstantin Lopuhin\n",
    "    # https://www.kaggle.com/lopuhin/dstl-satellite-imagery-feature-detection/full-pipeline-demo-poly-pixels-ml-poly\n",
    "    img_mask = np.zeros(im_size, np.uint8)\n",
    "    if not polygons:\n",
    "        return img_mask\n",
    "    int_coords = lambda x: np.array(x).round().astype(np.int32)\n",
    "    exteriors = [int_coords(poly.exterior.coords) for poly in polygons]\n",
    "    interiors = [int_coords(pi.coords) for poly in polygons\n",
    "                 for pi in poly.interiors]\n",
    "    cv2.fillPoly(img_mask, exteriors, 1)\n",
    "    cv2.fillPoly(img_mask, interiors, 0)\n",
    "    return img_mask\n",
    "\n",
    "\n",
    "def mask_to_polygons(mask, epsilon=EPSILON, min_area=MIN_AREA):\n",
    "    # __author__ = Konstantin Lopuhin\n",
    "    # https://www.kaggle.com/lopuhin/dstl-satellite-imagery-feature-detection/full-pipeline-demo-poly-pixels-ml-poly\n",
    "\n",
    "    # first, find contours with cv2: it's much faster than shapely\n",
    "    image, contours, hierarchy = cv2.findContours(\n",
    "        ((mask == 1) * 255).astype(np.uint8),\n",
    "        cv2.RETR_CCOMP, cv2.CHAIN_APPROX_TC89_KCOS)\n",
    "    # create approximate contours to have reasonable submission size\n",
    "    approx_contours = [cv2.approxPolyDP(cnt, epsilon, True)\n",
    "                       for cnt in contours]\n",
    "    if not contours:\n",
    "        return MultiPolygon()\n",
    "    # now messy stuff to associate parent and child contours\n",
    "    cnt_children = defaultdict(list)\n",
    "    child_contours = set()\n",
    "    assert hierarchy.shape[0] == 1\n",
    "    # http://docs.opencv.org/3.1.0/d9/d8b/tutorial_py_contours_hierarchy.html\n",
    "    for idx, (_, _, _, parent_idx) in enumerate(hierarchy[0]):\n",
    "        if parent_idx != -1:\n",
    "            child_contours.add(idx)\n",
    "            cnt_children[parent_idx].append(approx_contours[idx])\n",
    "    # create actual polygons filtering by area (removes artifacts)\n",
    "    all_polygons = []\n",
    "    for idx, cnt in enumerate(approx_contours):\n",
    "        if idx not in child_contours and cv2.contourArea(cnt) >= min_area:\n",
    "            assert cnt.shape[1] == 1\n",
    "            poly = Polygon(\n",
    "                shell=cnt[:, 0, :],\n",
    "                holes=[c[:, 0, :] for c in cnt_children.get(idx, [])\n",
    "                       if cv2.contourArea(c) >= min_area])\n",
    "            all_polygons.append(poly)\n",
    "    # approximating polygons might have created invalid ones, fix them\n",
    "    all_polygons = MultiPolygon(all_polygons)\n",
    "    if not all_polygons.is_valid:\n",
    "        all_polygons = all_polygons.buffer(0)\n",
    "        # Sometimes buffer() converts a simple Multipolygon to just a Polygon,\n",
    "        # need to keep it a Multi throughout\n",
    "        if all_polygons.type == 'Polygon':\n",
    "            all_polygons = MultiPolygon([all_polygons])\n",
    "    return all_polygons\n",
    "\n",
    "def get_scalers(im_size, x_max, y_min):\n",
    "    # __author__ = Konstantin Lopuhin\n",
    "    # https://www.kaggle.com/lopuhin/dstl-satellite-imagery-feature-detection/full-pipeline-demo-poly-pixels-ml-poly\n",
    "    h, w = im_size  # they are flipped so that mask_for_polygons works correctly\n",
    "    h, w = float(h), float(w)\n",
    "    w_ = 1.0 * w * (w / (w + 1))\n",
    "    h_ = 1.0 * h * (h / (h + 1))\n",
    "    return w_ / x_max, h_ / y_min\n",
    "\n",
    "def predict_id(id, model, trs):\n",
    "    # Read and preprocess\n",
    "    img = M(id)\n",
    "    x = stretch_n(img)\n",
    "\n",
    "    cnv = np.zeros((960, 960, 8)).astype(np.float32)\n",
    "    prd = np.zeros((N_Cls, 960, 960)).astype(np.float32)\n",
    "    cnv[:img.shape[0], :img.shape[1], :] = x\n",
    "\n",
    "    # NOTE 960/160 = 6\n",
    "    # for each col of a full image\n",
    "    for i in range(0, 6):\n",
    "        line = []\n",
    "        # predict each row of a full image at a time\n",
    "        for j in range(0, 6):\n",
    "            line.append(cnv[i * ISZ:(i + 1) * ISZ, j * ISZ:(j + 1) * ISZ])\n",
    "\n",
    "        x = 2 * np.transpose(line, (0, 3, 1, 2)) - 1 # Same sort of standardisation/normalisation as before (and th/tf ordering fix)\n",
    "        tmp = model.predict(x, batch_size=4)\n",
    "        for j in range(tmp.shape[0]):\n",
    "            prd[:, i * ISZ:(i + 1) * ISZ, j * ISZ:(j + 1) * ISZ] = tmp[j]\n",
    "\n",
    "    # trs = [0.4, 0.1, 0.4, 0.3, 0.3, 0.5, 0.3, 0.6, 0.1, 0.1]\n",
    "    for i in range(N_Cls):\n",
    "        prd[i] = prd[i] > trs[i]  # threshold \n",
    "\n",
    "    return prd[:, :img.shape[0], :img.shape[1]] # prediction for one image (and one class in my case)\n",
    "\n",
    "\n",
    "def predict_test(model, trs):\n",
    "    print(\"predict test\")\n",
    "    for i, id in enumerate(sorted(set(SB['ImageId'].tolist()))):\n",
    "        msk = predict_id(id, model, trs)\n",
    "        np.save(out_dir+'10_msk_%s' % id, msk)\n",
    "        if i % 100 == 0: print( i, id)\n",
    "\n",
    "\n",
    "def make_submit():\n",
    "    print(\"make submission file\")\n",
    "    df = pd.read_csv(os.path.join(data_base_dir+'sample_submission.csv'))\n",
    "    print(df.head())\n",
    "    for idx, row in df.iterrows():\n",
    "        if df.iloc[idx, 1] != 5:\n",
    "            df.iloc[idx, 2] = \"MULTIPOLYGON EMPTY\"\n",
    "            continue\n",
    "     \n",
    "        id = row[0]\n",
    "#         kls = row[1] - 1\n",
    "        kls = 0\n",
    "\n",
    "#         print(out_dir+'10_msk_%s.npy' % id)\n",
    "        msk = np.load(out_dir+'10_msk_%s.npy' % id)[kls]\n",
    "        pred_polygons = mask_to_polygons(msk, epsilon=EPSILON, min_area=MIN_AREA)\n",
    "        x_max = GS.loc[GS['ImageId'] == id, 'Xmax'].as_matrix()[0]\n",
    "        y_min = GS.loc[GS['ImageId'] == id, 'Ymin'].as_matrix()[0]\n",
    "\n",
    "        x_scaler, y_scaler = get_scalers(msk.shape, x_max, y_min)\n",
    "\n",
    "        scaled_pred_polygons = shapely.affinity.scale(pred_polygons, xfact=1.0 / x_scaler, yfact=1.0 / y_scaler,\n",
    "                                                      origin=(0, 0, 0))\n",
    "\n",
    "        df.iloc[idx, 2] = shapely.wkt.dumps(scaled_pred_polygons)\n",
    "        if idx % 100 == 0: print( idx)\n",
    "    print(df.head())\n",
    "    df.to_csv(out_dir+'/subm.csv', index=False)\n",
    "\n",
    "\n",
    "def check_predict(id, model_path):\n",
    "    model = get_unet()\n",
    "    model.load_weights(model_path)\n",
    "\n",
    "    msk = predict_id(id, model, trs)\n",
    "    img = M(id)\n",
    "\n",
    "    plt.figure()\n",
    "    ax1 = plt.subplot(131)\n",
    "    ax1.set_title('image ID:'+id)\n",
    "    ax1.imshow(img[:, :, 5], cmap=plt.get_cmap('gist_ncar'))\n",
    "    ax2 = plt.subplot(132)\n",
    "    ax2.set_title('predict bldg pixels')\n",
    "    ax2.imshow(msk[0], cmap=plt.get_cmap('gray'))\n",
    "    ax3 = plt.subplot(133)\n",
    "    ax3.set_title('predict bldg polygones')\n",
    "    ax3.imshow(mask_for_polygons(mask_to_polygons(msk[0], epsilon=1), img.shape[:2]), cmap=plt.get_cmap('gray'))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_test(model, trs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
